运行一下东西，选一两个讲
他会找一些代码去让你证明这是自己写的
# 第一次实验
## 对率回归
分类问题只有0、1
为了连续可导--》sigmoid函数
sigmoid函数（线性回归结果）--》对率回归函数

核心在梯度下降求目标函数的最优值
其中最重要的梯度下降法部分的具体过程在代码中进行了详细标注.其核心步骤如下:

1. 初始化步长与迭代次数
2. 利用beta+=h*delta_beta逐步迭代更新自变量beta

## K折
绘制的iris数据集各变量关系组合图，其横轴纵轴都是属性,对角线处图形是单变量的分布

## 实现线性判别分析

# 第二次实验
通过实验结果表明预剪枝在部分数据量较少的数据集上退化成了单节点,具有一定的欠拟合风险,同时在一些数据集上模型更简洁而效果不差于完全决策树,基于**奥卡姆剃刀**准则来看,预剪枝具有一定的应用价值,而后剪枝在绝大多数情况下表现较好,但耗时较长是其缺点.

代码是id3mine
输入: 训练集 $D=\left\{\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots,\left(\boldsymbol{x}_{m}, y_{m}\right)\right\}$;
		属性集 $A=\left\{a_{1}, a_{2}, \ldots, a_{d}\right\}$.
		过程: 函数 TreeGenerate $(D, A)$---使用递归进行决策树分支构建
		输出: 以 node 为根结点的一棵决策树
#### 未剪枝情况下的
root = decision_tree.TreeGenerate(train_data,depth_tree)  # 根据训练集生成决策树

TreeGenerate(D,A):
##### 生成一个新的根节点new_node

每个决策节点由归因,标签,子节点字典组成
![](attachments/Pasted%20image%2020220425171043.png)

##### 找到当前数据集的最优类标签并赋予new_node
信息增益是衡量划分属性优劣的
信息熵是衡量样本集合纯度的

**获得新分支的最佳归因**与划分值  
new_node.node_property, div_value = OptAttr(df)
为此需要
遍历属性,**计算某属性的信息增益**,最大的信息增益的属性是最优属性
def InfoGain(cin_sample, index):即信息增益是指利用该属性进行划分后的信息增益
![](attachments/Pasted%20image%2020220425172256.png)
对于连续变量与离散变量,有不同的计算属性的信息增益的方法:
1. 计算当前集合的信息熵
2. 对于离散变量(类别变量),遍历所有可能取值,利用信息熵计算信息增益
3. 对于连续变量,利用二分法,首先将属性值排序,然后遍历样本,获取潜在划分点,计算其作为划分时的子集熵和---划分产生两个子集,分别计算熵,然后加上占比的系数后求和

利用最佳归因进行划分,传入划分后的子集,划分值,即new_node.attr_down(划分)=TreeGenerate(df_v_r,depth-1)
![](attachments/Pasted%20image%2020220425174703.png)
最后要返回newnode.

递归完成后用root访问树

用pydotplus可视化决策树

### 预剪枝
在建树时对划分前后泛化性能进行评估,
与未剪枝的区别之处在:
预剪枝在划分后如果效果不好则回退并停止剪枝,结束递归
![](attachments/Pasted%20image%2020220425180322.png)

### 后剪枝
1. 正常生成树
2. 自底向上遍历树,得到tree_list
![](attachments/Pasted%20image%2020220425180630.png)
1. 自底向上遍历tree_list,不剪叶子节点,否则剪枝试试,如果剪枝后的精度小于剪枝前的精度，还原,否则下一个
### 基于基尼指数
区别在于最佳归因是使用的基尼指数,其他没有区别
# 第三次实验
## 标准梯度下降与累计BP的区别
标准梯度下降是在权值更新前对所有样例汇总误差，而随机梯度下降的权值是通过考查某个训练样例来更新的。

标准BP算法(每次只针对一个训练样例更新连接权):**随机梯度下降**
累积BP算法(累计误差最小化):**标准梯度下降**

具体两种情况的结果如下图：可以看出来gd的成本函数收敛过程更加稳定，而sgd每次迭代并不一定向最优方向前进，但总体方向是收敛的，且同样是迭代200次，最后结果相差不大，但由于sgd每次迭代只使用一个样本，计算量大幅度下降，显然sgd的速度会更快。

ps.关于随机梯度下降的实现，好像有两种方式，一种是每次将样本打乱，然后遍历所有样本，而后再次打乱、遍历；另一种是**每次迭代随机抽取样本**。这里采取的是后一种方式，貌似两种方式都可以。
## 过程
初始化

训练选用是神经元激活函数是sigmoid函数

向前传播:
![](attachments/Pasted%20image%2020220425160001.png)
        cache_ = (a_pre_, w_, b_, z_)  # 将向前传播过程中产生的数据保存下来，在向后传播过程计算梯度的时候要用上的。

反向传播:
X是输入样本的属性,y在本数据集中的值是0/1,a_last是最后一层激活函数的输出值,z是激活前的值
sigmoid函数的性质很重要
![](attachments/Pasted%20image%2020220425162617.png)

利用此前向前传播的cache值得到每一层的梯度

利用梯度与学习率更新参数

得到当前参数下的输出,计算损失函数

计算损失函数:
![](attachments/Pasted%20image%2020220425162341.png)