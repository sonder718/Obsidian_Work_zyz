4分钟
# 任务三
KNN基本步骤：
1.  计算待测数据点与所有训练数据的距离；
2.  距离值递增排序；
3.  选出前K个最小**距离**（欧氏距离）；
4.  统计这K个距离值所对应的标签的频数；
5.  频数最大的标签即为预测类别。

上述为KNN做分类预测的步骤。
![](attachments/Pasted%20image%2020220530165836.png)
# 澳大利亚下雨
## XGBoost
是一个树集成模型，它使用的是K个树的每棵树对样本的预测值**的和**作为该样本在XGBoost系统中的预测。
他基于提升树，一个模型表现不好，我继续按照原来模型表现不好的那部分训练第二个模型
它为处理稀疏数据使用了一个新颖的树学习算法

我们可以有不同的目标：
一，我希望不计一切代价判断出少数类，得到最高的recall。
二，我们希望追求最高的预测准确率，一切目的都是为了让accuracy更高，我们不在意recall或者AUC。
三，我们希望达到recall，ROC和accuracy之间的平衡，不追求任何一个也不牺牲任何一个。
## **召回率(Recall):**
表示的是`样本中的正例有多少被预测正确了（找得全）`所有正例中被正确预测出来的比例。
## **精确率(Precision):**
表示的是`预测为正的样本中有多少是真正的正样本（找得对）`。预测结果中真正的正例的比例。
## **ROC曲线与AUC：**
ROC（Receiver Operating Characteristic）曲线是以假正率（FP_rate）和真正率（TP_rate）为轴的曲线，[ROC曲线](https://so.csdn.net/so/search?q=ROC%E6%9B%B2%E7%BA%BF&spm=1001.2101.3001.7020)下面的面积我们叫做**AUC**
AUC只能用于二分类；AUC并不能应对严重的数据不平衡，如1：99这种，此时ROC会有突变的陡峭截面，使得其对数量少的样本不敏感，经验上AUC在平衡比例在1：5 以内的样本上都可以有不错表现。
AUC越接近1.0，检测方法真实性越高;等于0.5时，则真实性最低，无应用价值。